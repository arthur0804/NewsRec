{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script preprocesses the dataset.\n",
    "# 1. break the whole json file into seperate json files (each one represents a news)\n",
    "# 2. remove news that are identifies as not relevant to the task\n",
    "# 3. extract news id, title, content and other possible fields\n",
    "# 4. do a shallow cleaning of the content (only removing HTML tags, not removing stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shallow_clean(text):\n",
    "    '''\n",
    "    remove HTML tags or other special characters in the text\n",
    "    Args: some text\n",
    "    Returns: text being cleaned\n",
    "    '''\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_news_fields(news):\n",
    "    '''\n",
    "    processing text by extracting news id, title, content and do a shallow clean\n",
    "    Args: a news (dictionary) to be processed\n",
    "    Returns: a news (dictionary) after being processed\n",
    "    '''\n",
    "    news_parsed = {}\n",
    "\n",
    "    # extract id, title, author and\n",
    "    news_parsed[\"id\"] = news[\"id\"]\n",
    "    news_parsed[\"title\"] = news[\"title\"]\n",
    "    news_parsed[\"author\"] = news[\"author\"]\n",
    "\n",
    "    # parse contents (an array)\n",
    "    contents = news[\"contents\"]\n",
    "\n",
    "    # news type is the first element in the array and with the \"kicker\" indicator, but not necessarily exist so need to check\n",
    "    if contents[0][\"type\"] == \"kicker\":\n",
    "        news_category = contents[0][\"content\"]\n",
    "    else:\n",
    "        news_category = None\n",
    "    news_parsed[\"category\"] = news_category\n",
    "\n",
    "    # extract detailed content text from the array\n",
    "    content_text_list = []\n",
    "    for c in contents:\n",
    "        if c is not None and c[\"type\"] == \"sanitized_html\":\n",
    "            content_text = shallow_clean(c[\"content\"])\n",
    "            content_text_list.append(content_text)\n",
    "    news_parsed[\"content\"] = \" \".join(content_text_list)\n",
    "\n",
    "    return news_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_news_type(news):\n",
    "    '''\n",
    "    check whether the news is relevant to the task\n",
    "    Args: a news\n",
    "    Returns: a boolean value\n",
    "    '''\n",
    "    non_rel_news_types = [\"Opinion\", \"Letters to the Editor\", \"The Post's View\"]\n",
    "\n",
    "    # extract news type\n",
    "    if \"content\" in news[\"contents\"][0].keys():\n",
    "        news_type = news[\"contents\"][0][\"content\"]\n",
    "    else:\n",
    "        news_type = None\n",
    "\n",
    "    if news_type not in non_rel_news_types:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(file_path, folder_path):\n",
    "    '''\n",
    "    preprocess all the news\n",
    "    '''\n",
    "    print(\"Start processing.\")\n",
    "\n",
    "    # keep track of how many documents\n",
    "    total_news_count = 0\n",
    "    total_rel_news_count = 0\n",
    "\n",
    "    with open (file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            news = json.loads(line)\n",
    "            if check_news_type(news):\n",
    "                # extract information and save to the folder\n",
    "                news_parsed = extract_news_fields(news)\n",
    "                news_path = folder_path + news_parsed[\"id\"] + \".json\"\n",
    "                with open (news_path , \"w\", encoding=\"utf-8\") as fp:\n",
    "                    json.dump(news_parsed, fp, ensure_ascii=False)\n",
    "                total_rel_news_count += 1\n",
    "            total_news_count += 1\n",
    "            # checkpoint\n",
    "            if total_news_count % 1000 == 0:\n",
    "                print(\"{} news processed.\".format(total_news_count))\n",
    "        f.close()\n",
    "    print(\"{} news processed, {} relevant news saved.\".format(total_news_count, total_rel_news_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # be sure to change the file path and folder path (where you want to save the parsed news files)\n",
    "    file_path = \"/Users/jiamingqu/Desktop/NewsRecProj/data/wapo.jl\"\n",
    "    folder_path = \"/Users/jiamingqu/Desktop/NewsRecProj/news_json/\"\n",
    "    preprocessing(file_path, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    # it should be a total of 595,037 news, in which news are 571,963 relevant and kept for this task\n",
    "    # take approximately 8 min on my laptop to finish"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
